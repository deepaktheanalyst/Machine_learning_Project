{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b6d12170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os ,sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bbb28d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.magicbricks.com/property-for-rent/residential-real-estate?bedroom=&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment,Service-Apartment,Residential-House,Villa&cityName=Gurgaon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2ac68b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Use the webdriver.Chrome() constructor without the executable_path argument\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Define a function to scroll to the bottom of the page to load dynamic content\n",
    "    def scroll_to_bottom():\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(20)  # Adjust the waiting time as needed\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    # Scroll to the bottom of the page to load all content\n",
    "    scroll_to_bottom()\n",
    "\n",
    "    # Extract the HTML content after the dynamic content has been loaded\n",
    "    data = driver.page_source\n",
    "\n",
    "    # Create BeautifulSoup object for parsing\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Find all the items on the page (assuming each item has the same structure)\n",
    "    all_items = soup.find_all('div', attrs={'class': 'mb-srp__card'})\n",
    "\n",
    "    # Create a list to store all the dictionaries containing details for each item\n",
    "    all_details = []\n",
    "\n",
    "    # Loop through all the items\n",
    "    for item in all_items:\n",
    "        title = item.find('h2', attrs={'class': 'mb-srp__card--title'})\n",
    "        price = item.find('div', attrs={'class': 'mb-srp__card__price--amount'})\n",
    "        details = item.find_all('div', attrs={'class': 'mb-srp__card__summary--value'})\n",
    "        label = item.find_all('div', attrs={'class': 'mb-srp__card__summary--label'})\n",
    "\n",
    "        if title and price:  # Check if title and price are present\n",
    "            full_detail = {'title': title.text.strip(), 'price': price.text.strip()}\n",
    "        else:\n",
    "            continue  # Skip this item if title or price is missing\n",
    "\n",
    "        for i in range(min(len(details), len(label))):\n",
    "            key = label[i].text.strip()  # Use the label text as the key\n",
    "            value = details[i].text.strip()  # Use the details text as the value\n",
    "\n",
    "            if key in full_detail:  # If a key already exists, convert the value to a list\n",
    "                existing_value = full_detail[key]\n",
    "                if not isinstance(existing_value, list):\n",
    "                    full_detail[key] = [existing_value]\n",
    "                full_detail[key].append(value)\n",
    "            else:\n",
    "                full_detail[key] = value\n",
    "\n",
    "        all_details.append(full_detail)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle the exception\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver in the 'finally' block to ensure it's always closed\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f99388a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "274760e9-f954-4389-846b-baf1fdf8ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurgaon_data = pd.DataFrame(all_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bdff1ade-62fc-44ce-8fad-96b7f5b93fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lac_to_number(value):\n",
    "    match = re.match(r'^(\\d+(?:\\.\\d+)?)\\s*Lac$', value)\n",
    "    if match:\n",
    "        return int(float(match.group(1)) * 100000)\n",
    "    return value\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df[['title', 'price', 'Furnishing', 'Bathroom', 'Balcony']]\n",
    "    df['Balcony'].fillna(value='0', inplace=True)\n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    bhk = []\n",
    "    for index, row in df.iterrows():\n",
    "        b = row['title'].split('for')[0]\n",
    "        bhk.append(b.replace('  ', ''))\n",
    "    df['bhk'] = bhk\n",
    "    \n",
    "    locality = []\n",
    "    for index, row in df.iterrows():\n",
    "        a = row['title'].split('in', 1)[1]\n",
    "        locality.append(a)\n",
    "    df['locality'] = locality\n",
    "    \n",
    "    df.drop('title', axis=1, inplace=True)\n",
    "    \n",
    "    df['price'] = df['price'].str.replace('â‚¹', '')\n",
    "    df['price'] = [convert_lac_to_number(value) for value in df['price']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "19fcf89b-390e-4edf-b0b2-feea9b35087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurgaon_data = clean_df(gurgaon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0060efac-6374-4043-b76b-e4a04a972d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurgaon_data['locality']=gurgaon_data['locality'].str.replace('Gurgaon','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "902c14a9-ef64-4824-8801-9f7b0793e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurgaon_data['City'] = 'Gurgaon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8111350a-21f1-43db-864f-60c1555ad18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurgaon_data.to_csv('gurgaon_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca799d54-44f4-4056-b215-f8a9c5f7cb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e1ace-104f-48c0-b7a0-cdfcafbc38dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
